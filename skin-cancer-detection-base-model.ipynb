{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2275763,"sourceType":"datasetVersion","datasetId":1370616},{"sourceId":10377603,"sourceType":"datasetVersion","datasetId":6428263}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T09:09:26.688743Z","iopub.execute_input":"2025-01-22T09:09:26.689058Z","iopub.status.idle":"2025-01-22T09:09:36.099080Z","shell.execute_reply.started":"2025-01-22T09:09:26.689020Z","shell.execute_reply":"2025-01-22T09:09:36.098226Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.7)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.19.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T09:09:36.101850Z","iopub.execute_input":"2025-01-22T09:09:36.102606Z","iopub.status.idle":"2025-01-22T09:09:44.519274Z","shell.execute_reply.started":"2025-01-22T09:09:36.102567Z","shell.execute_reply":"2025-01-22T09:09:44.518143Z"}},"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nimport torch.nn.functional as F\nimport wandb\nfrom torchsummary import summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T09:09:44.520797Z","iopub.execute_input":"2025-01-22T09:09:44.521111Z","iopub.status.idle":"2025-01-22T09:09:50.241117Z","shell.execute_reply.started":"2025-01-22T09:09:44.521082Z","shell.execute_reply":"2025-01-22T09:09:50.240484Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, in_channels=3, out_channels=16, latent_dim=200):\n        super().__init__()\n        self.out_channels = out_channels\n\n        self.net = nn.Sequential( #  597x449\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),  # (600, 450)\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),  \n            nn.ReLU(),\n            nn.Conv2d(out_channels, 2 * out_channels, kernel_size=3, stride=2, padding=1),  # (298, 225)\n            nn.ReLU(),\n            nn.Conv2d(2 * out_channels, 2 * out_channels, kernel_size=3, padding=1),  \n            nn.ReLU(),\n            nn.Conv2d(2 * out_channels, 4 * out_channels, kernel_size=3, stride=2, padding=1),  # (150, 113)\n            nn.ReLU(),\n            nn.Conv2d(4 * out_channels, 4 * out_channels, kernel_size=3, padding=1),  \n            nn.ReLU(),\n        )\n        \n        self.flatten_size = 4 * out_channels * 150 * 113\n\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(self.flatten_size, latent_dim),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 3, 597, 449)\n        x = self.net(x)\n        x = self.fc(x)\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels=3, out_channels=16, latent_dim=200):\n        super().__init__()\n        self.out_channels = out_channels\n\n        self.fc = nn.Sequential(\n            nn.Linear(latent_dim, 4 * out_channels * 150 * 113),\n            nn.ReLU(),\n        )\n\n        self.conv = nn.Sequential(\n            nn.ConvTranspose2d(4 * out_channels, 4 * out_channels, kernel_size=3, padding=1),  # (150, 113)\n            nn.ReLU(),\n            nn.ConvTranspose2d(4 * out_channels, 2 * out_channels, kernel_size=3, stride=2, padding=1, output_padding=0),  # (300, 225)\n            nn.ReLU(),\n            nn.ConvTranspose2d(2 * out_channels, 2 * out_channels, kernel_size=3, padding=1),  \n            nn.ReLU(),\n            nn.ConvTranspose2d(2 * out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=0),  # (600, 450)\n            nn.ReLU(),\n            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(out_channels, in_channels, kernel_size=3, padding=1),  \n            nn.Sigmoid(),  # Normalization output [0, 1]\n        )\n\n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(-1, 4 * self.out_channels, 150, 113)  # Відновлюємо форму для згорткових шарів\n        x = self.conv(x)\n        return x\n\n\nclass Autoencoder(nn.Module):\n    def __init__(self, encoder, decoder, device=\"cuda\"):\n        super().__init__()\n        self.encoder = encoder\n        self.encoder.to(device)\n        self.decoder = decoder\n        self.decoder.to(device)\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T09:09:50.242269Z","iopub.execute_input":"2025-01-22T09:09:50.242613Z","iopub.status.idle":"2025-01-22T09:09:50.255349Z","shell.execute_reply.started":"2025-01-22T09:09:50.242575Z","shell.execute_reply":"2025-01-22T09:09:50.254543Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Function to calculate the correlation coefficient\ndef correlation_coefficient(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    numerator = np.sum((x - x_mean) * (y - y_mean))\n    denominator = np.sqrt(np.sum((x - x_mean)**2) * np.sum((y - y_mean)**2))\n    \n    return numerator / denominator\n\n# Function to calculate the threshold\ndef calculate_threshold(correlation_coeffs):\n    mu_c = np.mean(correlation_coeffs)\n    sigma_c = np.std(correlation_coeffs)\n    threshold = mu_c - 0.5 * sigma_c\n    return threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T09:09:50.256477Z","iopub.execute_input":"2025-01-22T09:09:50.256732Z","iopub.status.idle":"2025-01-22T09:09:50.268341Z","shell.execute_reply.started":"2025-01-22T09:09:50.256707Z","shell.execute_reply":"2025-01-22T09:09:50.267553Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!wandb login 492953ddcda0576b6e6ebf89860aed0ccd177efe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T09:09:50.269298Z","iopub.execute_input":"2025-01-22T09:09:50.269616Z","iopub.status.idle":"2025-01-22T09:09:53.081014Z","shell.execute_reply.started":"2025-01-22T09:09:50.269580Z","shell.execute_reply":"2025-01-22T09:09:53.079582Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"run_name = f\"Autoencoder all examples out_channels=8, latent_dim=32 batch_size=8\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:10:07.604476Z","iopub.execute_input":"2025-01-22T11:10:07.605183Z","iopub.status.idle":"2025-01-22T11:10:07.608951Z","shell.execute_reply.started":"2025-01-22T11:10:07.605150Z","shell.execute_reply":"2025-01-22T11:10:07.608085Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"wandb.init(\n    # set the wandb project where this run will be logged\n    project=\"Skin cancer BASE model\",\n\n    # track hyperparameters and run metadata\n    config={\n    \"epochs\" : 10,\n    \"batch_size\" : 8,\n    \"learning_rate\" : 0.002,\n    },\n    name=run_name\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:10:09.226666Z","iopub.execute_input":"2025-01-22T11:10:09.227612Z","iopub.status.idle":"2025-01-22T11:10:12.880028Z","shell.execute_reply.started":"2025-01-22T11:10:09.227565Z","shell.execute_reply":"2025-01-22T11:10:12.879237Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:g4ofu5sa) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <style>\n        .wandb-row {\n            display: flex;\n            flex-direction: row;\n            flex-wrap: wrap;\n            justify-content: flex-start;\n            width: 100%;\n        }\n        .wandb-col {\n            display: flex;\n            flex-direction: column;\n            flex-basis: 100%;\n            flex: 1;\n            padding: 10px;\n        }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▂▁▂▃▃▄▁▅</td></tr><tr><td>psnr</td><td>█▆▃▄▅▁▇▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.56348</td></tr><tr><td>psnr</td><td>-0.06377</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Autoencoder all examples out_channels=8, latent_dim=32 batch_size=16</strong> at: <a href='https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model/runs/g4ofu5sa' target=\"_blank\">https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model/runs/g4ofu5sa</a><br/> View project at: <a href='https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model' target=\"_blank\">https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250122_100208-g4ofu5sa/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:g4ofu5sa). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250122_111009-flz2owgx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model/runs/flz2owgx' target=\"_blank\">Autoencoder all examples out_channels=8, latent_dim=32 batch_size=8</a></strong> to <a href='https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model' target=\"_blank\">https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model/runs/flz2owgx' target=\"_blank\">https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model/runs/flz2owgx</a>"},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kkaterynabilyk-lviv-polytechnic-national-university/Skin%20cancer%20BASE%20model/runs/flz2owgx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7ece017a10c0>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# Class for loading images from a folder\nclass CustomDataset(Dataset):\n    def __init__(self, folder_path, transform=None, max_images=None):\n        self.image_paths = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path) if fname.endswith('.jpg') or fname.endswith('.png')]\n        if max_images is not None:\n            self.image_paths = self.image_paths[:max_images]  # Limit the number of images to max_images\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_name = self.image_paths[idx]\n        image = Image.open(img_name).convert('RGB')\n        image = np.array(image)\n        if self.transform:\n            # image = self.transform(image)\n            image = self.transform(image=image)['image']\n        return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:10:28.000456Z","iopub.execute_input":"2025-01-22T11:10:28.000828Z","iopub.status.idle":"2025-01-22T11:10:28.008500Z","shell.execute_reply.started":"2025-01-22T11:10:28.000798Z","shell.execute_reply":"2025-01-22T11:10:28.007669Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport wandb\nfrom skimage.metrics import structural_similarity as ssim\nimport math\n\ndef psnr(target, prediction):\n    \"\"\"\n    Calculate the PSNR between the target and prediction.\n    \"\"\"\n    mse = torch.mean((target - prediction) ** 2)\n    if mse == 0:\n        return float('inf')  # No error, images are identical\n    max_pixel = 1.0  # Assuming pixel values are normalized between [0, 1]\n    psnr_value = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n    return psnr_value\n\nfrom tqdm import tqdm\n\ndef train_autoencoder(model, dataloader, criterion, optimizer, scheduler, device, epochs):\n    \"\"\"\n    Train the autoencoder with batch support.\n    \"\"\"\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        total_psnr = 0.0\n\n        # Use tqdm for progress tracking\n        with tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n            for batch_idx, data in enumerate(pbar):\n                # Move data to the specified device (GPU/CPU)\n                data = data.to(device)\n                # print(data.min(), data.max())  # Should be 0 and 1\n\n\n                # Zero the gradients\n                optimizer.zero_grad()\n\n                # Perform the forward pass (prediction)\n                output = model(data)\n                \n                # Calculate the loss\n                loss = criterion(output, data)\n                loss.backward()\n                optimizer.step()\n\n                # Aggregate metrics\n                train_loss += loss.item()\n                psnr_value = psnr(data, output)\n                total_psnr += psnr_value.item()\n\n                # Update tqdm progress bar with current loss and PSNR\n                pbar.set_postfix(loss=loss.item(), psnr=psnr_value.item())\n\n        # Calculate average loss and PSNR for the epoch\n        average_loss = train_loss / len(dataloader)\n        average_psnr = total_psnr / len(dataloader)\n\n        # Log metrics to the console\n        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}, PSNR: {average_psnr:.2f}\")\n        wandb.log({\"epoch\": epoch + 1, \"loss\": average_loss, \"psnr\": average_psnr})\n\n        # Update the learning rate scheduler\n        scheduler.step(average_loss)\n\n    return model\n\n\n\n\n# def train_autoencoder(model, dataloader, criterion, optimizer, scheduler, device, epochs):\n#     \"\"\"\n#     Function to train the autoencoder.\n    \n#     Args:\n#         model: The autoencoder model.\n#         dataloader: The data loader.\n#         criterion: The loss function.\n#         optimizer: The optimizer.\n#         scheduler: The learning rate scheduler.\n#         device: The device (CPU or GPU).\n#         epochs: The number of epochs.\n\n#     Returns:\n#         The model after training.\n#     \"\"\"\n#     for epoch in range(epochs):\n#         model.train()\n#         train_loss = 0.0\n#         total_psnr = 0.0\n#         total_ssim = 0.0\n        \n#         for data in dataloader:\n#             data = data.to(device)\n            \n            \n#             optimizer.zero_grad()\n#             output = model(data)\n            \n#             # Resizing to avoid shape mismatch error\n#             data_resized = F.interpolate(data, size=(597, 449), mode='bicubic', align_corners=False)\n            \n#             loss = criterion(output, data_resized)\n#             loss.backward()\n#             print(loss)\n#             optimizer.step()\n\n#             train_loss += loss.item()\n\n#             # Calculate PSNR and SSIM\n#             psnr_value = psnr(data_resized, output)\n#             total_psnr += psnr_value.item()\n\n#             # ssim_value = ssim(\n#             # data_resized.cpu().numpy().transpose(0, 2, 3, 1), \n#             # output.cpu().detach().numpy().transpose(0, 2, 3, 1), \n#             # win_size=7,  # Explicitly set the window size\n#             # channel_axis=-1  # Indicate that the last dimension is the channel axis\n#             # )\n#             # total_ssim += np.mean(ssim_value)\n\n#         # Average loss, PSNR, and SSIM for the epoch\n#         average_loss = train_loss / len(dataloader)\n#         average_psnr = total_psnr / len(dataloader)\n#         # average_ssim = total_ssim / len(dataloader)\n        \n#         # # Output results\n#         # print(f\"Epoch [{epoch+1}/{epochs}], Loss: {average_loss}, PSNR: {average_psnr}, SSIM: {average_ssim}\")\n        \n#         # # Logging the metrics to wandb\n#         # wandb.log({\"epoch\": epoch + 1, \"loss\": average_loss, \"psnr\": average_psnr, \"ssim\": average_ssim})\n\n#         # Output results\n#         print(f\"Epoch [{epoch+1}/{epochs}], Loss: {average_loss}, PSNR: {average_psnr}\")\n        \n#         # Logging the metrics to wandb\n#         wandb.log({\"epoch\": epoch + 1, \"loss\": average_loss, \"psnr\": average_psnr})\n        \n        \n#         # Update learning rate\n#         scheduler.step(average_loss)\n\n#     return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T10:02:31.135707Z","iopub.execute_input":"2025-01-22T10:02:31.136167Z","iopub.status.idle":"2025-01-22T10:02:31.153727Z","shell.execute_reply.started":"2025-01-22T10:02:31.136127Z","shell.execute_reply":"2025-01-22T10:02:31.152585Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"encoder = Encoder(in_channels=3, out_channels=8, latent_dim=16)\ndecoder = Decoder(in_channels=3, out_channels=8, latent_dim=16)\nautoencoder_model = Autoencoder(encoder, decoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T10:02:36.746942Z","iopub.execute_input":"2025-01-22T10:02:36.747286Z","iopub.status.idle":"2025-01-22T10:02:36.931526Z","shell.execute_reply.started":"2025-01-22T10:02:36.747245Z","shell.execute_reply":"2025-01-22T10:02:36.930845Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True if GPU is available\nprint(torch.cuda.current_device())  # Should return the index of the current GPU\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T10:02:38.188112Z","iopub.execute_input":"2025-01-22T10:02:38.188750Z","iopub.status.idle":"2025-01-22T10:02:38.194561Z","shell.execute_reply.started":"2025-01-22T10:02:38.188712Z","shell.execute_reply":"2025-01-22T10:02:38.193733Z"}},"outputs":[{"name":"stdout","text":"True\n0\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"summary(autoencoder_model, (3, 597, 449))  # 597x449","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T10:02:39.782798Z","iopub.execute_input":"2025-01-22T10:02:39.783133Z","iopub.status.idle":"2025-01-22T10:02:39.816894Z","shell.execute_reply.started":"2025-01-22T10:02:39.783105Z","shell.execute_reply":"2025-01-22T10:02:39.815994Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1          [-1, 8, 597, 449]             224\n              ReLU-2          [-1, 8, 597, 449]               0\n            Conv2d-3          [-1, 8, 597, 449]             584\n              ReLU-4          [-1, 8, 597, 449]               0\n            Conv2d-5         [-1, 16, 299, 225]           1,168\n              ReLU-6         [-1, 16, 299, 225]               0\n            Conv2d-7         [-1, 16, 299, 225]           2,320\n              ReLU-8         [-1, 16, 299, 225]               0\n            Conv2d-9         [-1, 32, 150, 113]           4,640\n             ReLU-10         [-1, 32, 150, 113]               0\n           Conv2d-11         [-1, 32, 150, 113]           9,248\n             ReLU-12         [-1, 32, 150, 113]               0\n          Flatten-13               [-1, 542400]               0\n           Linear-14                   [-1, 16]       8,678,416\n             ReLU-15                   [-1, 16]               0\n          Encoder-16                   [-1, 16]               0\n           Linear-17               [-1, 542400]       9,220,800\n             ReLU-18               [-1, 542400]               0\n  ConvTranspose2d-19         [-1, 32, 150, 113]           9,248\n             ReLU-20         [-1, 32, 150, 113]               0\n  ConvTranspose2d-21         [-1, 16, 299, 225]           4,624\n             ReLU-22         [-1, 16, 299, 225]               0\n  ConvTranspose2d-23         [-1, 16, 299, 225]           2,320\n             ReLU-24         [-1, 16, 299, 225]               0\n  ConvTranspose2d-25          [-1, 8, 597, 449]           1,160\n             ReLU-26          [-1, 8, 597, 449]               0\n  ConvTranspose2d-27          [-1, 8, 597, 449]             584\n             ReLU-28          [-1, 8, 597, 449]               0\n  ConvTranspose2d-29          [-1, 3, 597, 449]             219\n          Sigmoid-30          [-1, 3, 597, 449]               0\n          Decoder-31          [-1, 3, 597, 449]               0\n================================================================\nTotal params: 17,935,555\nTrainable params: 17,935,555\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 3.07\nForward/backward pass size (MB): 252.23\nParams size (MB): 68.42\nEstimated Total Size (MB): 323.72\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Data preparation\nimage_folder = \"/kaggle/input/ham1000-segmentation-and-classification/images\"  # Path to the folder containing images\nimport albumentations as A\nimport cv2\nfrom albumentations.pytorch import ToTensorV2\n\ntransform = A.Compose([\n    A.Resize(height=597, width=449),\n    A.Blur(blur_limit=3, p=0.5),  \n    A.ShiftScaleRotate(shift_limit=0.1, rotate_limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT), \n    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.05, p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\n\ndataset = CustomDataset(image_folder, transform)\ndataloader = DataLoader(dataset, batch_size=wandb.config.batch_size, shuffle=True)\n\n# # Creating and training the autoencoder\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nautoencoder = autoencoder_model.to(device)\noptimizer = optim.Adam(autoencoder.parameters(), lr=wandb.config.learning_rate)\ncriterion = nn.BCEWithLogitsLoss() \nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n\n# Calling the training function\nautoencoder = train_autoencoder(\n    model=autoencoder,\n    dataloader=dataloader,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    device=device,\n    epochs=wandb.config.epochs\n)\n\n# Saving the model\ntorch.save(autoencoder.state_dict(), \"cnn_autoencoder.pth\")\nwandb.save(\"cnn_autoencoder.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:10:37.100913Z","iopub.execute_input":"2025-01-22T11:10:37.101279Z","iopub.status.idle":"2025-01-22T11:53:50.816334Z","shell.execute_reply.started":"2025-01-22T11:10:37.101248Z","shell.execute_reply":"2025-01-22T11:53:50.815371Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 1252/1252 [04:19<00:00,  4.82batch/s, loss=0.534, psnr=-0.35]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 0.5618, PSNR: -0.01\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 1252/1252 [04:19<00:00,  4.83batch/s, loss=0.642, psnr=-0.0791]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 0.5613, PSNR: -0.01\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 1252/1252 [04:19<00:00,  4.82batch/s, loss=0.537, psnr=-0.563]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Loss: 0.5601, PSNR: 0.01\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 1252/1252 [04:19<00:00,  4.82batch/s, loss=0.632, psnr=-0.91]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 0.5605, PSNR: 0.01\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 1252/1252 [04:19<00:00,  4.83batch/s, loss=0.553, psnr=-1.32]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Loss: 0.5619, PSNR: -0.02\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 1252/1252 [04:20<00:00,  4.81batch/s, loss=0.511, psnr=-0.56]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Loss: 0.5622, PSNR: -0.03\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 1252/1252 [04:18<00:00,  4.84batch/s, loss=0.558, psnr=1.55]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 0.5598, PSNR: 0.01\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 1252/1252 [04:19<00:00,  4.83batch/s, loss=0.566, psnr=-0.604]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 0.5624, PSNR: -0.00\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 1252/1252 [04:19<00:00,  4.83batch/s, loss=0.55, psnr=0.284]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Loss: 0.5603, PSNR: 0.01\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 1252/1252 [04:18<00:00,  4.84batch/s, loss=0.522, psnr=2.02]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Loss: 0.5622, PSNR: -0.02\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/wandb/run-20250122_111009-flz2owgx/files/cnn_autoencoder.pth']"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# # Отримання вихідного зображення після реконструкції\n# test_image = Image.open(\"/kaggle/input/dddddsd/photo_5_2025-01-05_14-16-42.jpg\").convert('RGB')\n# test_image = transform(test_image).unsqueeze(0).to(device)\n# output_image = autoencoder(test_image).detach().cpu().numpy()\n\n# # Обчислення коефіцієнту кореляції\n# test_image_resized = transforms.functional.resize(test_image, size=(600, 456), interpolation=transforms.InterpolationMode.BICUBIC)\n# input_pixels = test_image_resized.squeeze(0).cpu().numpy().flatten()\n\n# reconstructed_pixels = output_image.flatten()\n# corr_value = correlation_coefficient(input_pixels, reconstructed_pixels)\n# print(f\"Correlation Coefficient: {corr_value}\")\n\n# # Масив кореляцій для класу (приклад)\n# correlation_coeffs = np.random.rand(100)  # Масив кореляцій для класу\n\n# # Обчислення порогу для цього класу\n# threshold = calculate_threshold(correlation_coeffs)\n# print(f\"Threshold: {threshold}\")\n\n# # Перевірка чи зображення належить тому ж класу\n# if corr_value > threshold:\n#     print(\"Зображення належить тому ж класу.\")\n# else:\n#     print(\"Зображення не належить тому ж класу.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T22:38:48.996313Z","iopub.status.idle":"2025-01-20T22:38:48.996786Z","shell.execute_reply.started":"2025-01-20T22:38:48.996533Z","shell.execute_reply":"2025-01-20T22:38:48.996557Z"}},"outputs":[],"execution_count":null}]}